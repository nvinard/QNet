{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5K6wSQ2WgeB"
   },
   "source": [
    "# Continuous transfer learning\n",
    "\n",
    "## Update QNet on a regular (daily) basis\n",
    "\n",
    "Start with updated QNet using 4 days of data.\n",
    "Next, update using all events predicted by QNet on day 5 using locations given by catalog. Now use days 1-5 including field data to update model (20-40 epochs) select best model based on dice-coef. Apply yo day 6 and repeat the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wgGtadoEXcrH"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import random\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8TAaVV5Zm5T"
   },
   "source": [
    "## Data augmentations and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MWWybIHdWfpO"
   },
   "outputs": [],
   "source": [
    "# The data augmentaiton function defined in this block are only used for the synthetic data\n",
    "\n",
    "def random_t_shifts(d):\n",
    "   for i in range(d.shape[1]):\n",
    "     d[:,i] = np.roll(d[:,i], np.random.choice(1024))\n",
    "   return d\n",
    "\n",
    "def full_field_noise(data):\n",
    "\n",
    "  # pick random time-window\n",
    "  noise_idx = random.choice(np.arange(cfg.field_noise.shape[0]))\n",
    "  idx = random.choice(np.arange(0, cfg.field_noise.shape[1]-cfg.N_TIMESAMPLES))\n",
    "  noise = cfg.field_noise[noise_idx, idx:idx+cfg.N_TIMESAMPLES, :]\n",
    "\n",
    "  noise = np.random.uniform(4.0,12.0)*normalize(noise)\n",
    "  data = normalize(data)+noise\n",
    "\n",
    "  return data\n",
    "\n",
    "def tf_full_field_noise(data: tf.Tensor) -> tf.Tensor:\n",
    "  data_shape = data.shape\n",
    "  [data,] = tf.py_function(full_field_noise, [data], [tf.float32])\n",
    "  data.set_shape(data_shape)\n",
    "  return data\n",
    "\n",
    "def station_dropout(data):\n",
    "    n_drop = random.randrange(cfg.MIN_DROP, cfg.MAX_DROP, 1)\n",
    "    drop_stations = np.random.choice(np.arange(data.shape[1]), n_drop, replace=False)\n",
    "    mask = np.ones((data.shape))\n",
    "    mask[:, drop_stations] *= 0\n",
    "    data = np.multiply(data, mask)\n",
    "    return data\n",
    "\n",
    "def tf_station_dropout(data: tf.Tensor) -> tf.Tensor:\n",
    "    data_shape = data.shape\n",
    "    [data,] = tf.py_function(station_dropout, [data], [tf.float32])\n",
    "    data.set_shape(data_shape)\n",
    "    return data\n",
    "\n",
    "def time_shift(data, z):\n",
    "    negative_time_shifts = np.array([-100, -150, -200, -240, -270, -310])*0.7 #np.array([-20, -30, -40, -48, -54, -62]) #np.array([-100, -150, -200, -240, -270, -310])\n",
    "    positive_time_shifts = np.array([550, 500, 450, 400, 350, 300])*0.7 #np.array([120, 112, 104, 96, 88, 80]) #np.array([600, 560, 520, 480, 440, 400])\n",
    "    \n",
    "    ind_label = np.argmax(z)\n",
    "    t_shift = np.random.randint(negative_time_shifts[ind_label], positive_time_shifts[ind_label])\n",
    "  \n",
    "    data = np.roll(data, t_shift, axis=0)\n",
    "\n",
    "    if t_shift < 0:\n",
    "        data[t_shift:, :] = 0\n",
    "    elif t_shift > 0:\n",
    "        data[:t_shift, :] = 0\n",
    "\n",
    "    return data\n",
    "\n",
    "def tf_time_shift(data, z):\n",
    "    data_shape = data.shape\n",
    "    [data,] = tf.py_function(time_shift, [data, z], [tf.float32])\n",
    "    data.set_shape(data_shape)\n",
    "    return data\n",
    "\n",
    "def normalize(data):\n",
    "    data = data/np.max(np.abs(data))\n",
    "    return data\n",
    "\n",
    "def tf_normalize(data):\n",
    "    return tf.math.divide(data, tf.math.reduce_max(tf.math.abs(data)))\n",
    "\n",
    "def FCN_output(sx):\n",
    "    x = np.reshape(np.linspace(5500,8500,128), (128,1,1)) #np.reshape(np.linspace(5300,8700,88), (88,1,1))\n",
    "    y = np.reshape(np.linspace(3500,6100,96), (1,96,1)) #np.reshape(np.linspace(3300,6300,48), (1,48,1\n",
    "    z = np.reshape(np.linspace(1400,3600,64), (1,1,64)) # np.reshape(np.linspace(1200,3800,32), (1,1,32)\n",
    "    xc = np.round(sx[0])\n",
    "    yc = np.round(sx[1])\n",
    "    zc = np.round(sx[2])\n",
    "    fcn_out = np.exp(-((x-xc)**2/(2*200**2)+(y-yc)**2/(2*200**2)+(z-zc)**2/(2*200**2)))#np.exp(-((x-xc)**2+(y-yc)**2+(z-zc)**2)/(2*300**2))#np.exp(-((x-xc)**2/(2*200**2)+(y-yc)**2/(2*200**2)+(z-zc)**2/(2*200**2)))\n",
    "    fcn_out = fcn_out/np.max(fcn_out)\n",
    "    return fcn_out\n",
    "\n",
    "def tf_FCN_output(xc,yc,zc):\n",
    "    [fcn,] = tf.py_function(FCN_output, [[xc,yc,zc]], [tf.float32])\n",
    "    fcn.set_shape((128,96,64))\n",
    "    return fcn\n",
    "\n",
    "def FCN_output_perturbed(sx):\n",
    "    x = np.reshape(np.linspace(5500,8500,128), (128,1,1)) #np.reshape(np.linspace(5300,8700,88), (88,1,1))\n",
    "    y = np.reshape(np.linspace(3500,6100,96), (1,96,1)) #np.reshape(np.linspace(3300,6300,48), (1,48,1\n",
    "    z = np.reshape(np.linspace(1400,3600,64), (1,1,64)) # np.reshape(np.linspace(1200,3800,32), (1,1,32)\n",
    "\n",
    "    loc_errors = [sx[3], sx[4], sx[5]]\n",
    "\n",
    "    rx = np.random.normal(loc=sx[0], scale=np.std(np.arange(-int(loc_errors[0]),int(loc_errors[0])+1)))\n",
    "    ry = np.random.normal(loc=sx[1], scale=np.std(np.arange(-int(loc_errors[1]),int(loc_errors[1])+1)))\n",
    "    rz = np.random.normal(loc=sx[2], scale=np.std(np.arange(-int(loc_errors[2]),int(loc_errors[2])+1)))\n",
    "\n",
    "    while abs(rx-sx[0]) > loc_errors[0]:\n",
    "      rx = np.random.normal(loc=sx[0], scale=np.std(np.arange(-int(loc_errors[0]),int(loc_errors[0])+1)))\n",
    "    while abs(ry-sx[1]) > loc_errors[1]:\n",
    "      ry = np.random.normal(loc=sx[1], scale=np.std(np.arange(-int(loc_errors[1]),int(loc_errors[1])+1)))\n",
    "    while abs(rz-sx[2]) > loc_errors[2]:\n",
    "      rz = np.random.normal(loc=sx[2], scale=np.std(np.arange(-int(loc_errors[2]),int(loc_errors[2])+1)))\n",
    "\n",
    "    xc = rx\n",
    "    yc = ry\n",
    "    zc = rz\n",
    "\n",
    "    fcn_out = np.exp(-((x-xc)**2/(2*200**2)+(y-yc)**2/(2*200**2)+(z-zc)**2/(2*200**2)))#np.exp(-((x-xc)**2+(y-yc)**2+(z-zc)**2)/(2*300**2))#np.exp(-((x-xc)**2/(2*200**2)+(y-yc)**2/(2*200**2)+(z-zc)**2/(2*200**2)))\n",
    "    fcn_out = fcn_out/np.max(fcn_out)\n",
    "    return fcn_out\n",
    "\n",
    "def tf_FCN_output_perturbed(xc,yc,zc, rx, ry, rz):\n",
    "    [fcn,] = tf.py_function(FCN_output, [[xc,yc,zc, rx, ry, rz]], [tf.float32])\n",
    "    fcn.set_shape((128,96,64))\n",
    "    return fcn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DB8BkXzxZsH1"
   },
   "source": [
    "## TFR reading pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FFbLFIV-YXXb"
   },
   "outputs": [],
   "source": [
    "# Create an input function reading a file using the Dataset API\n",
    "def read_dataset(prefix, batch_size, config):\n",
    "  def _input_fn(example_serialized):\n",
    "    feature_map = {\n",
    "        \"x_true\": tf.io.FixedLenFeature(shape=[], dtype=tf.float32, default_value=-1),\n",
    "        \"y_true\": tf.io.FixedLenFeature(shape=[], dtype=tf.float32, default_value=-1),\n",
    "        \"z_true\": tf.io.FixedLenFeature(shape=[], dtype=tf.float32, default_value=-1),\n",
    "        \"sx_error\": tf.io.FixedLenFeature(shape=[], dtype=tf.float32, default_value=-1),\n",
    "        \"sy_error\": tf.io.FixedLenFeature(shape=[], dtype=tf.float32, default_value=-1),\n",
    "        \"sz_error\": tf.io.FixedLenFeature(shape=[], dtype=tf.float32, default_value=-1),\n",
    "        \"z_id\": tf.io.FixedLenFeature(shape=[6], dtype=tf.float32),\n",
    "        \"data\": tf.io.FixedLenFeature(shape=[config.N_TIMESAMPLES, config.N_TRACES], dtype=tf.float32),\n",
    "        }\n",
    "\n",
    "    parsed = tf.io.parse_single_example(example_serialized, feature_map)\n",
    "    data = parsed[\"data\"]\n",
    "\n",
    "    if tf.math.reduce_max(parsed[\"z_id\"])>=0:\n",
    "      data = tf_time_shift(data, parsed[\"z_id\"])\n",
    "      data = tf_full_field_noise(data)\n",
    "      data = tf_station_dropout(data)\n",
    "      label = tf_FCN_output(parsed[\"x_true\"], parsed[\"y_true\"], parsed[\"z_true\"])\n",
    "    else:\n",
    "      label = tf_FCN_output(parsed[\"x_true\"], parsed[\"y_true\"], parsed[\"z_true\"])\n",
    "      #label = tf_FCN_output_perturbed(parsed[\"x_true\"], parsed[\"y_true\"], parsed[\"z_true\"],\n",
    "      #                                parsed[\"sx_error\"], parsed[\"sy_error\"], parsed[\"sz_error\"])\n",
    "\n",
    "    data = tf_normalize(data)\n",
    "    data = tf.reshape(data, (config.N_TIMESAMPLES, config.N_TRACES, 1))\n",
    "    \n",
    "    return (data, label)\n",
    "\n",
    "  def _input_fn2(example_serialized):\n",
    "    feature_map = {\n",
    "        \"x_true\": tf.io.FixedLenFeature(shape=[], dtype=tf.float32, default_value=-1),\n",
    "        \"y_true\": tf.io.FixedLenFeature(shape=[], dtype=tf.float32, default_value=-1),\n",
    "        \"z_true\": tf.io.FixedLenFeature(shape=[], dtype=tf.float32, default_value=-1),\n",
    "        \"z_id\": tf.io.FixedLenFeature(shape=[6], dtype=tf.float32),\n",
    "        \"data\": tf.io.FixedLenFeature(shape=[config.N_TIMESAMPLES, config.N_TRACES], dtype=tf.float32),\n",
    "        }\n",
    "\n",
    "    parsed = tf.io.parse_single_example(example_serialized, feature_map)\n",
    "    data = parsed[\"data\"]\n",
    "\n",
    "    label = tf_FCN_output(parsed[\"x_true\"], parsed[\"y_true\"], parsed[\"z_true\"])\n",
    "\n",
    "    data = tf_normalize(data)\n",
    "    data = tf.reshape(data, (config.N_TIMESAMPLES, config.N_TRACES, 1))\n",
    "    \n",
    "    return (data, label)\n",
    "\n",
    "  if prefix==\"train\":\n",
    "\n",
    "    # Get synthetic and field data\n",
    "    file_path_synth = os.path.join(config.PATH_TO_SYNTH_DATA,'%s*' % prefix)\n",
    "    file_path_field = os.path.join(config.PATH_TO_FIELD_DATA, '%s*' % prefix)\n",
    "\n",
    "    # Create list of files that match pattern and process synthetic dataset\n",
    "    file_list_synth = tf.io.matching_files(file_path_synth)\n",
    "    file_list_synth = tf.random.shuffle(file_list_synth)\n",
    "    file_list_field = tf.io.matching_files(file_path_field)\n",
    "    if config.TRANSFER_LEARNING == True:\n",
    "      file_list_synth = tf.slice(file_list_synth, [0], [config.N_SYNTH_FILES])\n",
    "      file_list = tf.concat([file_list_synth, file_list_field], axis=-1) # combine field and synthetic data list\n",
    "      shards = tf.data.Dataset.from_tensor_slices(file_list)\n",
    "      shards = shards.shuffle(tf.cast(tf.shape(file_list)[0], tf.int64))\n",
    "      shards = shards.repeat()\n",
    "    else:\n",
    "      file_list = tf.concat([file_list_synth, file_list_field], axis=-1)\n",
    "      shards = tf.data.Dataset.from_tensor_slices(file_list)\n",
    "      shards = shards.shuffle(tf.cast(tf.shape(file_list)[0], tf.int64))\n",
    "      shards = shards.repeat()\n",
    "    # Feed the shards into TFRecordDataset and randomize again with interleave.\n",
    "    dataset = shards.interleave(tf.data.TFRecordDataset, cycle_length=4, block_length=16, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(buffer_size=10*batch_size)\n",
    "    num_epochs=None\n",
    "    dataset = dataset.map(_input_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "\n",
    "\n",
    "  if prefix == \"val\":\n",
    "    file_path_field = os.path.join(config.PATH_TO_FIELD_DATA, '%s*' % prefix)\n",
    "    file_list_field = tf.io.matching_files(file_path_field)\n",
    "    shards = tf.data.Dataset.from_tensor_slices(file_list_field)\n",
    "    dataset = tf.data.TFRecordDataset(shards)\n",
    "    num_epochs = 1\n",
    "    dataset = dataset.map(_input_fn2, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "\n",
    "  return dataset.prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtsSK0E5ZyQE"
   },
   "source": [
    "## Model updating/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HsylB76jY_9E"
   },
   "outputs": [],
   "source": [
    "def dice_coef_gaussian(y_true, y_pred, smooth=1):\n",
    "  y_true = tf.dtypes.cast(y_true>0.1, tf.int32)\n",
    "  y_true = tf.dtypes.cast(y_true, tf.float32)\n",
    "  y_pred = tf.dtypes.cast(y_pred>0.1, tf.int32)\n",
    "  y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "  intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
    "  union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n",
    "  dice = K.mean((2. * intersection + smooth)/(union + smooth), axis=0)\n",
    "  return dice\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "  y_true = tf.dtypes.cast(y_true, tf.float32)\n",
    "  y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "  intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
    "  union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n",
    "  dice = K.mean((2. * intersection + smooth)/(union + smooth), axis=0)\n",
    "  return dice\n",
    "\n",
    "# Clip values in Gaussian above 0.1 to 1 and below 0.1 to 0\n",
    "def iou_coef_gaussian(y_true, y_pred, smooth=1):\n",
    "    y_true = tf.dtypes.cast(y_true>0.1, tf.int32)\n",
    "    y_true = tf.dtypes.cast(y_true, tf.float32)\n",
    "    y_pred = tf.dtypes.cast(y_pred>0.1, tf.int32)\n",
    "    y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=[1,2,3])\n",
    "    union = K.sum(y_true,[1,2,3])+K.sum(y_pred,[1,2,3])-intersection\n",
    "    iou = K.mean((intersection + smooth) / (union + smooth), axis=0)\n",
    "    return iou\n",
    "\n",
    "def train_and_evaluate(config):\n",
    "\n",
    "  # create checkpoint to save best model based on dice coef in validation set\n",
    "  filepath = os.path.join(config.PATH_TO_CKPTS, 'ckpt')\n",
    "  checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=filepath, \n",
    "                             monitor='val_dice_coef',\n",
    "                             save_weights_only=True,\n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "\n",
    "  print(\"loading model...\")\n",
    "  generator = tf.keras.models.load_model(\n",
    "      os.path.join(config.PATH_TO_MODELS, config.LOAD_MODEL),\n",
    "      custom_objects={\n",
    "          'ReLU':tf.keras.layers.ReLU,\n",
    "          'dice_coef_gaussian': dice_coef_gaussian, 'dice_coef': dice_coef,\n",
    "          'iou_coef_gaussian': iou_coef_gaussian,\n",
    "          })\n",
    "  \n",
    "  generator.compile(\n",
    "      optimizer=tf.keras.optimizers.Adam(learning_rate=config.LEARNING_RATE),\n",
    "      loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "      metrics=[dice_coef_gaussian, dice_coef])\n",
    "\n",
    "  # Compute train and validation steps\n",
    "  file_path = os.path.join(config.PATH_TO_SYNTH_DATA,'%s*' % config.TRAIN)\n",
    "  file_list = tf.io.matching_files(file_path)\n",
    "\n",
    "  file_list = file_list[:config.N_SYNTH_FILES]\n",
    "  n_train_examples_synth = len(file_list)*config.N_FILES_SYNTH_TFRECORD\n",
    "  \n",
    "  file_path = os.path.join(config.PATH_TO_FIELD_DATA, '%s*' % config.TRAIN)\n",
    "  file_list = tf.io.matching_files(file_path)\n",
    "  n_train_files_field = len(file_list)*config.N_FILES_FIELD_TFRECORD_TRAIN\n",
    "\n",
    "  n_train_examples = n_train_files_field + n_train_examples_synth\n",
    "\n",
    "  steps_per_epoch = n_train_examples // config.BATCH_SIZE\n",
    "\n",
    "  file_path = file_path = os.path.join(config.PATH_TO_FIELD_DATA, '%s*' % config.VAL)\n",
    "  file_list = tf.io.matching_files(file_path)\n",
    "  val_steps = (len(file_list)*config.N_FILES_FIELD_TFRECORD_VAL) // config.BATCH_SIZE\n",
    "\n",
    "\n",
    "\n",
    "  #trainds = read_dataset(prefix=config.TRAIN, batch_size=config.BATCH_SIZE, config=config)\n",
    "  #valds = read_dataset(prefix=config.VAL, batch_size=config.BATCH_SIZE, config=config)\n",
    "\n",
    "  #callbacks = [checkpoint]\n",
    "  #generator.fit(\n",
    "  #    trainds,\n",
    "  #    validation_data=valds,\n",
    "  #    epochs=config.N_EPOCHS,\n",
    "  #    steps_per_epoch=steps_per_epoch,\n",
    "  #    validation_steps=val_steps,\n",
    "  #    verbose=1,\n",
    "  #    callbacks=callbacks)\n",
    "  #\n",
    "  ## After training load best model and save it to desired directory\n",
    "  #generator.load_weights(filepath)\n",
    "  #new_idx = config.model_idx + 1\n",
    "  #config.model_idx = new_idx\n",
    "  #fn_new_model = \"QNet_updated_{}.h5\".format(config.model_idx)\n",
    "  #config.LOAD_MODEL = fn_new_model\n",
    "  #generator.save(os.path.join(config.PATH_TO_MODELS, fn_new_model), overwrite=True, include_optimizer=True)\n",
    "  \n",
    "  for i in range(config.N_ITS):\n",
    "    # Load the training data\n",
    "    trainds = read_dataset(prefix=config.TRAIN, batch_size=config.BATCH_SIZE, config=config)\n",
    "    valds = read_dataset(prefix=config.VAL, batch_size=config.BATCH_SIZE, config=config)\n",
    "    generator.fit(\n",
    "        trainds,\n",
    "        validation_data=valds,\n",
    "        epochs=config.N_EPOCHS,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_steps=val_steps,\n",
    "        verbose=1,)\n",
    "    \n",
    "    save_as = os.path.join(config.PATH_TO_TL_MODELS, \"{}_{}.h5\".format(config.MODEL_NAME, i+82))\n",
    "    generator.save(save_as, overwrite=True, include_optimizer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dawQwbIQZ2bs"
   },
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yqWdUS3gaK8-"
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_validations(data_files, model):\n",
    "\n",
    "    '''\n",
    "\n",
    "    pd = (data_files, model, pd_dict_name, detection_threshold=0.6)\n",
    "\n",
    "    Runs model prediction for all data_files and returns file names of the detected events,\n",
    "    list of detections (True/False) and a list of all the filenames.\n",
    "    Additionally it generates pandas dataframe with information about the prediction for each event.\n",
    "\n",
    "    Args:\n",
    "        data_files: list of paths to input data\n",
    "        model: model used for predictions\n",
    "        pd_dict_name: name of dictionary to save\n",
    "        detection_threshold: detection threshold\n",
    "\n",
    "    Retuns:\n",
    "        pd: pandas dataframe (also saved to disk)\n",
    "\n",
    "    '''\n",
    "    \n",
    "    F1, F1_binary = [], []\n",
    "    hypo_dist, epi_dist, depth_dist = [], [], []\n",
    "    max_val = []\n",
    "\n",
    "    x = np.reshape(np.linspace(5500,8500,128), (128,))\n",
    "    y = np.reshape(np.linspace(3500,6100,96), (96,))\n",
    "    z = np.reshape(np.linspace(1400,3600,64), (64,))\n",
    "    \n",
    "    for i, fn in enumerate(data_files):\n",
    "\n",
    "        ds = xr.open_dataset(fn)\n",
    "\n",
    "        data = ds['data'].values\n",
    "        dt = 0.002\n",
    "        t = np.arange(0,1401)*dt\n",
    "        tn = np.linspace(0,t[-1],1024)\n",
    "        dtn = tn[1]-tn[0]\n",
    "        data_new = np.zeros((len(tn),data.shape[1]))\n",
    "\n",
    "        for i in range(data_new.shape[1]):\n",
    "            data_new[:,i] = np.interp(tn, t, data[:,i])\n",
    "\n",
    "        data = data_new\n",
    "        sx_new = ds.attrs['source_coordinates']\n",
    "      \n",
    "        data = normalize(data)\n",
    "        data = np.reshape(data, (1, data.shape[0], data.shape[1],1))\n",
    "        prediction = model.predict(data)#[0,:]\n",
    "        desired = FCN_output(sx_new)\n",
    "        desired = np.reshape(desired, (1, desired.shape[0], desired.shape[1], desired.shape[2]))\n",
    "\n",
    "        # Compute fisher ratios data_tf = tf.convert_to_tensor(data_np, np.float32)\n",
    "        F1.append(dice_coef(tf.convert_to_tensor(desired, np.float32), tf.convert_to_tensor(prediction, np.float32)).numpy())\n",
    "        F1_binary.append(dice_coef_gaussian(tf.convert_to_tensor(desired, np.float32), tf.convert_to_tensor(prediction, np.float32)).numpy())\n",
    "\n",
    "        prediction = prediction[0,:]\n",
    "        maxIdx = np.unravel_index(np.argmax(prediction), prediction.shape)\n",
    "        xp = x[maxIdx[0]]\n",
    "        yp = y[maxIdx[1]]\n",
    "        zp = z[maxIdx[2]]\n",
    "          \n",
    "        max_val.append(np.max(prediction))\n",
    "\n",
    "        hypo_dist.append(np.sqrt((sx_new[0]-x[maxIdx[0]])**2+(sx_new[1]-y[maxIdx[1]])**2+(sx_new[2]-z[maxIdx[2]])**2))\n",
    "        epi_dist.append(np.sqrt((sx_new[0]-x[maxIdx[0]])**2+(sx_new[1]-y[maxIdx[1]])**2))\n",
    "        depth_dist.append(np.sqrt((sx_new[2]-z[maxIdx[2]])**2))\n",
    "\n",
    "    return F1, F1_binary, hypo_dist, epi_dist, depth_dist, max_val\n",
    "\n",
    "def model_eval(config):\n",
    "\n",
    "  F1 = []\n",
    "  model_name = []\n",
    "\n",
    "  for i in range(config.N_ITS):\n",
    "\n",
    "    # Load model\n",
    "    path_model = os.path.join(config.PATH_TO_TL_MODELS, config.MODEL_NAME + '_{}.h5'.format(i))\n",
    "    model = tf.keras.models.load_model(\n",
    "        path_model,\n",
    "        custom_objects={\n",
    "            'LeakyReLU':tf.keras.layers.LeakyReLU, 'ReLU':tf.keras.layers.ReLU,\n",
    "            'dice_coef_gaussian': dice_coef_gaussian, 'dice_coef': dice_coef,\n",
    "            'iou_coef_gaussian': iou_coef_gaussian,\n",
    "        })\n",
    "    \n",
    "    files = glob.glob(cfg.PATH_TO_FIELD_DATA_NC + '/*')\n",
    "    F1_tmp, F1_binary_tmp, hypo_dist_tmp, epi_dist_tmp, depth_dist_tmp, max_val_tmp = run_validations(files, model)\n",
    "    F1.append(np.mean(F1_tmp))\n",
    "\n",
    "  print(F1)\n",
    "  print(len(F1))\n",
    "  mod_idx = np.argmax(F1)\n",
    "\n",
    "  mod_name = config.MODEL_NAME + '_{}'.format(mod_idx)\n",
    "\n",
    "  return mod_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uppUCuaDgbAI"
   },
   "source": [
    "## Load QNetPrev updated on 4 days of field data and make predictions on day 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "21M-T8jRgitf"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "files = glob.glob(\"drive/My Drive/Texas_TL/Data/contNoise_newShape/*\")\n",
    "\n",
    "#field_noise = np.load(files[0])\n",
    "#n_size = field_noise.shape\n",
    "#field_noise = np.reshape(field_noise, (1, n_size[0], n_size[1]))\n",
    "#for i, fn in enumerate(files[1:]):\n",
    "#  field_noise = np.append(field_noise, np.reshape(np.load(fn), (1,n_size[0], n_size[1])), axis=0)\n",
    "\n",
    "class Config(object):\n",
    "  def __init__(self):\n",
    "    # PATHS TO MODELS AND OUTPUT\n",
    "    self.PATH_TO_SYNTH_DATA = \"drive/My Drive/Texas_TL/Data/TFR_new\"\n",
    "    self.PATH_TO_FIELD_DATA = \"drive/My Drive/Texas_TL/Data/daily_updating_TFRs\"#_days4\"\n",
    "    self.PATH_TO_FIELD_DATA_NC = \"drive/My Drive/Texas_TL/Data/daily_updating_xarray\"\n",
    "    self.PATH_TO_MODELS = \"drive/My Drive/Texas_TL/TL_models/TL_TFR_regular_updating\"\n",
    "    self.PATH_TO_TL_MODELS = \"drive/My Drive/Texas_TL/TL_models/TL_TFR_all_temp\"\n",
    "    self.PATH_TO_CKPTS = 'drive/My Drive/Texas_TL/TL_models/ckpt'\n",
    "    # DEFINE SIZE OF INPUT, BATCH SIZE, NUMBER OF EPOCHS (EVALS), LEARNING RATE, OTHER FIXED NUMBERS\n",
    "    self.BATCH_SIZE = 10\n",
    "    self.N_TRACES = 96\n",
    "    self.N_TIMESAMPLES = 1024\n",
    "    self.N_EPOCHS = 4\n",
    "    self.N_ITS = 99\n",
    "    self.LEARNING_RATE =  0.001\n",
    "    self.N_FILES_SYNTH_TFRECORD = 400\n",
    "    self.N_FILES_FIELD_TFRECORD_TRAIN = 1\n",
    "    self.N_FILES_FIELD_TFRECORD_VAL = 1\n",
    "    self.MIN_DROP = 5\n",
    "    self.MAX_DROP = 25\n",
    "    # PREFIX FOR TRAINING AND VALIDATION SETS\n",
    "    self.TRAIN = \"train\"\n",
    "    self.VAL = \"val\"\n",
    "    # LOAD FIELD NOISE, SINGLE FIELD NOISE AND GAUSSIAN NOISE\n",
    "    self.field_noise = field_noise\n",
    "    # NAME FOR CHECKPOINTS\n",
    "    self.LOAD_MODEL = \"QNet_update_81.h5\" # name of model to load before starting training\n",
    "    self.MODEL_NAME = \"QNet_update\"\n",
    "    self.model_idx = 0\n",
    "    self.TRANSFER_LEARNING = True\n",
    "    self.N_SYNTH_FILES = 6\n",
    "    self.detection_threshold = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9AbwMichg-O5"
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_predictions(data_files, model, pd_dict_name, check_day, detection_threshold=0.6):\n",
    "\n",
    "    '''\n",
    "\n",
    "    pd = (data_files, model, pd_dict_name, detection_threshold=0.6)\n",
    "\n",
    "    Runs model prediction for all data_files and returns file names of the detected events,\n",
    "    list of detections (True/False) and a list of all the filenames.\n",
    "    Additionally it generates pandas dataframe with information about the prediction for each event.\n",
    "\n",
    "    Args:\n",
    "        data_files: list of paths to input data\n",
    "        model: model used for predictions\n",
    "        pd_dict_name: name of dictionary to save\n",
    "        detection_threshold: detection threshold\n",
    "\n",
    "    Retuns:\n",
    "        pd: pandas dataframe (also saved to disk)\n",
    "\n",
    "    '''\n",
    "    \n",
    "    distances_old, distances_new = [], []\n",
    "    true_x_new, true_y_new, true_z_new, pred_x, pred_y, pred_z = [], [], [], [], [], []\n",
    "    true_x_old, true_y_old, true_z_old = [], [], []\n",
    "    x_error, y_error, z_error = [], [], []\n",
    "    snr, magnitude = [], []\n",
    "    vol, clvd, dc = [], [], []\n",
    "    file_name = []\n",
    "    detection = []\n",
    "    fns = []\n",
    "    max_val = []\n",
    "    day = []\n",
    "\n",
    "    x = np.reshape(np.linspace(5500,8500,128), (128,))\n",
    "    y = np.reshape(np.linspace(3500,6100,96), (96,))\n",
    "    z = np.reshape(np.linspace(1400,3600,64), (64,))\n",
    "    \n",
    "    print(\"making predictions on days >= {} and < {}\".format(check_day[0], check_day[1]))\n",
    "    for i, fn in enumerate(data_files):\n",
    "\n",
    "        ds = xr.open_dataset(fn)\n",
    "\n",
    "        #if ds.attrs['day'] == check_day:\n",
    "        if ds.attrs['day'] >= check_day[0] and ds.attrs['day'] <  check_day[1]:\n",
    "\n",
    "          data = ds['data'].values\n",
    "          dt = 0.002\n",
    "          t = np.arange(0,1401)*dt\n",
    "          tn = np.linspace(0,t[-1],1024)\n",
    "          dtn = tn[1]-tn[0]\n",
    "          data_new = np.zeros((len(tn),data.shape[1]))\n",
    "          for i in range(data_new.shape[1]):\n",
    "              data_new[:,i] = np.interp(tn, t, data[:,i])\n",
    "          data = data_new\n",
    "          sx_old = ds.attrs['source_coordinates_new']\n",
    "          sx_new = ds.attrs['source_coordinates']\n",
    "          sx_error = ds.attrs['loc_errors']\n",
    "          \n",
    "          true_x_new.append(sx_new[0])\n",
    "          true_y_new.append(sx_new[1])\n",
    "          true_z_new.append(sx_new[2])\n",
    "          true_x_old.append(sx_old[0])\n",
    "          true_y_old.append(sx_old[1])\n",
    "          true_z_old.append(sx_old[2])\n",
    "          x_error.append(sx_error[0])\n",
    "          y_error.append(sx_error[1])\n",
    "          z_error.append(sx_error[2])\n",
    "          snr.append(ds.attrs['SNR'])\n",
    "          magnitude.append(ds.attrs['Magnitude'])\n",
    "          vol.append(ds.attrs['VOL'])\n",
    "          clvd.append(ds.attrs['CLVD'])\n",
    "          dc.append(ds.attrs['DC'])\n",
    "          file_name.append(fn.split('/')[-1])\n",
    "          fns.append(fn)\n",
    "          day.append(ds.attrs['day'])\n",
    "          \n",
    "          data = normalize(data)\n",
    "          data = np.reshape(data, (1, data.shape[0], data.shape[1],1))\n",
    "          prediction = model.predict(data)[0,:]\n",
    "      \n",
    "          maxIdx = np.unravel_index(np.argmax(prediction), prediction.shape)\n",
    "          xp = x[maxIdx[0]]\n",
    "          yp = y[maxIdx[1]]\n",
    "          zp = z[maxIdx[2]]\n",
    "          \n",
    "          pred_x.append(xp)\n",
    "          pred_y.append(yp)\n",
    "          pred_z.append(zp)\n",
    "          max_val.append(np.max(prediction))\n",
    "          \n",
    "          dist_old = np.sqrt((sx_old[0]-x[maxIdx[0]])**2+(sx_old[1]-y[maxIdx[1]])**2+(sx_old[2]-z[maxIdx[2]])**2)\n",
    "          dist_new = np.sqrt((sx_new[0]-x[maxIdx[0]])**2+(sx_new[1]-y[maxIdx[1]])**2+(sx_new[2]-z[maxIdx[2]])**2)\n",
    "          \n",
    "          distances_old.append(dist_old)\n",
    "          distances_new.append(dist_new)\n",
    "\n",
    "          if np.max(prediction) >= detection_threshold:\n",
    "              detection.append(True)\n",
    "          else:\n",
    "              detection.append(False)\n",
    "            \n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            'file': file_name,\n",
    "            'detection': detection,\n",
    "            'distances_old': distances_old,\n",
    "            'distances_new': distances_new,\n",
    "            'new_true_x': true_x_new,\n",
    "            'new_true_y': true_y_new,\n",
    "            'new_true_z': true_z_new,\n",
    "            'old_true_x': true_x_old,\n",
    "            'old_true_y': true_y_old,\n",
    "            'old_true_z': true_z_old,\n",
    "            'pred_x': pred_x,\n",
    "            'pred_y': pred_y,\n",
    "            'pred_z': pred_z,\n",
    "            'max_val': max_val,\n",
    "            'x_error': x_error,\n",
    "            'y_error': y_error,\n",
    "            'z_error': z_error,\n",
    "            'SNR': snr,\n",
    "            'Magnitude': magnitude,\n",
    "            'Volume': vol,\n",
    "            'DC': dc,\n",
    "            'CLVD': clvd,\n",
    "            'filenames': fns,\n",
    "            'day': day\n",
    "        })\n",
    "    \n",
    "    df.to_csv(pd_dict_name)\n",
    "\n",
    "    n_detections = sum(bool(x) for x in detection)\n",
    "    print(\"Number of events detected: {}\".format(n_detections))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uStl8pJgncR_"
   },
   "source": [
    "## Split predicted into a train and val set\n",
    "\n",
    "Write training set to tfrecords and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8gJWw4KinDg_"
   },
   "outputs": [],
   "source": [
    "def train_val_split(df):\n",
    "\n",
    "  df = df[df['max_val']>=0.7]\n",
    "  X = df[['filenames','new_true_x','new_true_y','new_true_z', 'x_error', 'y_error', 'z_error']]\n",
    "  y = df['filenames']\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2)\n",
    "  \n",
    "  # Write train and val set to tfrecords\n",
    "  writeTFRs(X_train, 'drive/My Drive/Texas_TL/Data/daily_updating_TFRs', 'train')\n",
    "  writeTFRs(X_test, 'drive/My Drive/Texas_TL/Data/daily_updating_TFRs', 'val')\n",
    "\n",
    "  # Copy val_fns\n",
    "  for i, fn in enumerate(y_test.to_list()):\n",
    "    shutil.copy(fn, 'drive/My Drive/Texas_TL/Data/daily_updating_xarray/{}'.format(fn.split('/')[-1]),  follow_symlinks=True)\n",
    "\n",
    "  \n",
    "  \n",
    "def get_data(fn):\n",
    "\n",
    "  ds = xr.open_dataset(fn)\n",
    "  data = ds.data.values.astype('float32')\n",
    "  data = interp_time(data)\n",
    "  \n",
    "  return data\n",
    "\n",
    "def interp_time(data):\n",
    "  dt = 0.002\n",
    "  t = np.arange(0,1401)*dt\n",
    "  tn = np.linspace(0,t[-1],1024)\n",
    "  dtn = tn[1]-tn[0]\n",
    "  x_new = np.zeros((len(tn),data.shape[1]))\n",
    "  for i in range(x_new.shape[1]):\n",
    "    x_new[:,i] = np.interp(tn, t, data[:,i])\n",
    "\n",
    "  return x_new\n",
    "\n",
    "def parser(data, label):\n",
    "  data = np.reshape(data, (data.shape[0]*data.shape[1]), order=\"C\")\n",
    "\n",
    "  parsed_data = {\n",
    "      \"data\": data,\n",
    "      \"z_id\": np.array([-1,-1,-1,-1,-1,-1]),\n",
    "      \"sx\": label[\"new_true_x\"].values[0],\n",
    "      \"sy\": label[\"new_true_y\"].values[0],\n",
    "      \"sz\": label[\"new_true_z\"].values[0],\n",
    "      \"sx_error\": label[\"x_error\"].values[0],\n",
    "      \"sy_error\": label[\"y_error\"].values[0],\n",
    "      \"sz_error\": label[\"z_error\"].values[0],\n",
    "  }\n",
    "\n",
    "  return parsed_data\n",
    "\n",
    "\n",
    "def get_tensor_object(single_gather):\n",
    "  tensor = tf.train.Example(\n",
    "      features = tf.train.Features(\n",
    "          feature={\n",
    "              \"data\": tf.train.Feature(\n",
    "                  float_list=tf.train.FloatList(value=single_gather[\"data\"])\n",
    "              ),\n",
    "              \"z_id\": tf.train.Feature(\n",
    "                  float_list=tf.train.FloatList(value=single_gather[\"z_id\"])\n",
    "              ),\n",
    "              \"x_true\": tf.train.Feature(\n",
    "                  float_list=tf.train.FloatList(value=[single_gather['sx']])\n",
    "              ),\n",
    "              \"y_true\": tf.train.Feature(\n",
    "                  float_list=tf.train.FloatList(value=[single_gather['sy']])\n",
    "              ),\n",
    "              \"z_true\": tf.train.Feature(\n",
    "                  float_list=tf.train.FloatList(value=[single_gather['sz']])\n",
    "              ),\n",
    "              \"sx_error\": tf.train.Feature(\n",
    "                  float_list=tf.train.FloatList(value=[single_gather['sx_error']])\n",
    "              ),\n",
    "              \"sy_error\": tf.train.Feature(\n",
    "                  float_list=tf.train.FloatList(value=[single_gather['sy_error']])\n",
    "              ),\n",
    "              \"sz_error\": tf.train.Feature(\n",
    "                  float_list=tf.train.FloatList(value=[single_gather['sz_error']])\n",
    "              ),\n",
    "          }\n",
    "      )\n",
    "  )\n",
    "  return tensor\n",
    "\n",
    "def writeTFRs(xy_info, output_dir, split):\n",
    "\n",
    "  n_files = len(os.listdir(output_dir))\n",
    "  for i, fn in enumerate(xy_info['filenames'].values):\n",
    "\n",
    "    k = n_files+i\n",
    "\n",
    "    with tf.io.TFRecordWriter(os.path.join(output_dir, '{}_field_data_{}.tfrecord'.format(split, k))) as tfwriter:\n",
    "\n",
    "      data = get_data(fn)\n",
    "      parsed_data = parser(data, xy_info[i:i+1])\n",
    "      record_tensor = get_tensor_object(parsed_data)\n",
    "      tfwriter.write(record_tensor.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMMKQFwo65tz"
   },
   "source": [
    "0) empty folders that need to be filled on the fly. for example the daily updating xarray folder.  Then fill it with the data it needs to begin with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MOKl-AM9htSk"
   },
   "outputs": [],
   "source": [
    "! rm drive/My\\Drive/Texas_TL/Data/daily_updating_xarray/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZm_LoM3h1mb"
   },
   "outputs": [],
   "source": [
    "def fill_xarray(files):\n",
    "  for i, fn in enumerate(files):\n",
    "    ds = xr.open_dataset(fn)\n",
    "    if ds.attrs['day'] < 20:\n",
    "      shutil.copy(fn, 'drive/My Drive/Texas_TL/Data/daily_updating_xarray/{}'.format(fn.split('/')[-1]),  follow_symlinks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5nERjlncih0w"
   },
   "outputs": [],
   "source": [
    "fill_xarray(test_fns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJLB8Jtnm6Mc"
   },
   "source": [
    "1) Load QNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hXfysEtum9c6",
    "outputId": "ff1d9ef9-70c8-478e-d63a-4f52a0396e65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model: drive/My Drive/Texas_TL/TL_models/TL_TFR_regular_updating/QNet_0.h5\n"
     ]
    }
   ],
   "source": [
    "cfg = Config()\n",
    "\n",
    "train_fns = glob.glob('drive/My Drive/cnn_gpu/texas_dataset' + '/train*.nc')\n",
    "val_fns = glob.glob('drive/My Drive/cnn_gpu/texas_dataset' + '/val*.nc')\n",
    "test_fns = glob.glob('drive/My Drive/cnn_gpu/texas_dataset' + '/test*.nc')\n",
    "train_fns = np.append(train_fns,val_fns)\n",
    "\n",
    "# files that are allways used to make predictions on the following day\n",
    "field_data_files = np.append(train_fns,test_fns)\n",
    "\n",
    "print(\"load model: {}\".format(os.path.join(cfg.PATH_TO_MODELS, cfg.LOAD_MODEL)))\n",
    "QNet = tf.keras.models.load_model(\n",
    "      os.path.join(cfg.PATH_TO_MODELS, cfg.LOAD_MODEL),\n",
    "      custom_objects={\n",
    "          'ReLU':tf.keras.layers.ReLU,\n",
    "          'dice_coef_gaussian': dice_coef_gaussian, 'iou_coef_gaussian': iou_coef_gaussian,\n",
    "          'dice_coef': dice_coef,\n",
    "          })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWMxubwKnA0a"
   },
   "source": [
    "2) Apply to following day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "JChUnzvtnACy",
    "outputId": "f5c01485-d60d-4441-9bdb-3f091e27764e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making predictions on days >= 20 and < 23\n",
      "Number of events detected: 285\n"
     ]
    }
   ],
   "source": [
    "df_day = make_predictions(data_files=field_data_files, model=QNet, pd_dict_name='tmp.csv', check_day=(20,23), detection_threshold=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fsQTb1BnT3B"
   },
   "source": [
    "3) Split predictions that passed threshold to train/val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "JvMtIh4Jndfz"
   },
   "outputs": [],
   "source": [
    "train_val_split(df_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWdvpCNnnwq9"
   },
   "source": [
    "4) Update QNet using field data up to current day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gZAuwLQhn2GN",
    "outputId": "a0cd67e4-2cbf-414d-978e-604cba0f1f27"
   },
   "outputs": [],
   "source": [
    "cfg = Config()\n",
    "print(cfg.LOAD_MODEL, cfg.model_idx)\n",
    "train_and_evaluate(cfg)\n",
    "print(cfg.LOAD_MODEL, cfg.model_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3KJZDYtois9"
   },
   "source": [
    "5) Evaluate best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "o6rfdJbxx8to",
    "outputId": "a1f334e8-7527-4188-8382-fc74d9ed9f26"
   },
   "outputs": [],
   "source": [
    "cfg = Config()\n",
    "best_model = model_eval(cfg)\n",
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCmfFvL1kelR"
   },
   "source": [
    "6) Copy best model to new file and rename model to load in config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "L4wIreCS4oX9"
   },
   "outputs": [],
   "source": [
    "def best_model_update(config, model_name, update_idx):\n",
    "\n",
    "  # rename model in config file\n",
    "  fn_new_model = \"QNet_updated_{}.h5\".format(update_idx)\n",
    "  config.LOAD_MODEL = fn_new_model\n",
    "\n",
    "  # copy model to model update dir\n",
    "  model_to_copy = os.path.join(config.PATH_TO_TL_MODELS, model_name+'.h5')\n",
    "  model_fn_dst = os.path.join(config.PATH_TO_MODELS, fn_new_model)\n",
    "  print(model_to_copy)\n",
    "  print(model_fn_dst)\n",
    "  shutil.copy(model_to_copy, model_fn_dst, follow_symlinks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "clC-IsJdrb1W",
    "outputId": "e90f97c7-1165-4c16-88e6-5047fbee1a49"
   },
   "outputs": [],
   "source": [
    "best_model_update(cfg, best_model, 69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64H2YXV1Lzxc"
   },
   "source": [
    "# Full model updating iteration code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "q664vCZ5CLwC",
    "outputId": "5024110e-a35d-4ae4-ecce-58fe15b99b96"
   },
   "outputs": [],
   "source": [
    "cfg = Config()\n",
    "\n",
    "train_fns = glob.glob('drive/My Drive/cnn_gpu/texas_dataset' + '/train*.nc')\n",
    "val_fns = glob.glob('drive/My Drive/cnn_gpu/texas_dataset' + '/val*.nc')\n",
    "test_fns = glob.glob('drive/My Drive/cnn_gpu/texas_dataset' + '/test*.nc')\n",
    "train_fns = np.append(train_fns,val_fns)\n",
    "\n",
    "# files that are allways used to make predictions on the following day\n",
    "field_data_files = np.append(train_fns,test_fns)\n",
    "\n",
    "\n",
    "def full_iterative_model_updating(config, field_data_files, update_days):\n",
    "\n",
    "  for i, day in enumerate(update_days[:-1]):\n",
    "\n",
    "    print(\"load model: \" + config.LOAD_MODEL)\n",
    "    QNet = tf.keras.models.load_model(\n",
    "      os.path.join(config.PATH_TO_MODELS, config.LOAD_MODEL),\n",
    "      custom_objects={\n",
    "          'ReLU':tf.keras.layers.ReLU,\n",
    "          'dice_coef_gaussian': dice_coef_gaussian, 'iou_coef_gaussian': iou_coef_gaussian,\n",
    "          'dice_coef': dice_coef,\n",
    "          })\n",
    "\n",
    "    # Make predictions \"real-time monitoring\"\n",
    "    df_day = make_predictions(field_data_files,QNet, 'tmp.csv', (day, update_days[i+1]), config.detection_threshold)\n",
    "\n",
    "    # Based on predictions that were made that passed threshold we'd update the location with a classic method offline\n",
    "    # Based on those refined event locations we create a labelled dataset to enirch the current field dataset\n",
    "    train_val_split(df_day)\n",
    "\n",
    "    # Set a timer to ensure new field training set is done saving the files\n",
    "    time.sleep(120)\n",
    "\n",
    "    # Using the enriched field dataset we update our model\n",
    "    train_and_evaluate(config)\n",
    "\n",
    "full_iterative_model_updating(cfg, field_data_files, update_days=[20,21,22,23,24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QDrUOIIphlpk"
   },
   "outputs": [],
   "source": [
    "! rm drive/My\\Drive/Texas_TL/Data/daily_updating_TFRs/train_field_data_*\n",
    "! rm drive/My\\Drive/Texas_TL/Data/daily_updating_TFRs/val_field_data_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zqdU_rFmSAD7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "continuous_TL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
