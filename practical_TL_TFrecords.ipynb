{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ZCM997CYeBE"
   },
   "outputs": [],
   "source": [
    "# The data augmentaiton function defined in this block are only used for the synthetic data\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def random_t_shifts(d):\n",
    "   for i in range(d.shape[1]):\n",
    "     d[:,i] = np.roll(d[:,i], np.random.choice(1024))\n",
    "   return d\n",
    "\n",
    "def full_field_noise(data):\n",
    "\n",
    "  #files = glob.glob(cfg.PATH_FIELD_NOISE+'/*')\n",
    "  #fn = random.choice(files)\n",
    "  #noise = np.load(fn)\n",
    "\n",
    "  # pick random time-window\n",
    "  noise_idx = random.choice(np.arange(cfg.field_noise.shape[0]))\n",
    "  idx = random.choice(np.arange(0, cfg.field_noise.shape[1]-cfg.N_TIMESAMPLES))\n",
    "  noise = cfg.field_noise[noise_idx, idx:idx+cfg.N_TIMESAMPLES, :]\n",
    "  #idx = random.choice(np.arange(0,noise.shape[0]-cfg.N_TIMESAMPLES))\n",
    "  #noise = cfg.field_noise[noise_idx, idx:idx+cfg.N_TIMESAMPLES, :]\n",
    "\n",
    "  #noise = noise[idx:idx+cfg.N_TIMESAMPLES, :]\n",
    "  #noise = normalize(noise)\n",
    "  noise = np.random.uniform(1.0,8.0)*normalize(noise)\n",
    "  #if random.choice([True, False]) == True:\n",
    "  #  noise = np.flipud(noise)\n",
    "  #if random.choice([True, False]) == True:\n",
    "  #  noise = np.random.permutation(noise.T).T\n",
    "  #if random.choice([True, False]) == True:\n",
    "  #  noise = random_t_shifts(noise)\n",
    "  \n",
    "  #noise = np.reshape(noise, (noise.shape[0], noise.shape[1], 1))\n",
    "  data = normalize(data)+noise\n",
    "\n",
    "  return data\n",
    "\n",
    "def tf_full_field_noise(data: tf.Tensor) -> tf.Tensor:\n",
    "  data_shape = data.shape\n",
    "  [data,] = tf.py_function(full_field_noise, [data], [tf.float32])\n",
    "  data.set_shape(data_shape)\n",
    "  return data\n",
    "\n",
    "def station_dropout(data):\n",
    "    n_drop = random.randrange(cfg.MIN_DROP, cfg.MAX_DROP, 1)\n",
    "    drop_stations = np.random.choice(np.arange(data.shape[1]), n_drop, replace=False)\n",
    "    mask = np.ones((data.shape))\n",
    "    mask[:, drop_stations] *= 0\n",
    "    data = np.multiply(data, mask)\n",
    "    return data\n",
    "\n",
    "def tf_station_dropout(data: tf.Tensor) -> tf.Tensor:\n",
    "    data_shape = data.shape\n",
    "    [data,] = tf.py_function(station_dropout, [data], [tf.float32])\n",
    "    data.set_shape(data_shape)\n",
    "    return data\n",
    "\n",
    "def time_shift(data, z):\n",
    "    negative_time_shifts = np.array([-100, -150, -200, -240, -270, -310])*0.7 #np.array([-20, -30, -40, -48, -54, -62]) #np.array([-100, -150, -200, -240, -270, -310])\n",
    "    positive_time_shifts = np.array([550, 500, 450, 400, 350, 300])*0.7 #np.array([120, 112, 104, 96, 88, 80]) #np.array([600, 560, 520, 480, 440, 400])\n",
    "    \n",
    "    ind_label = np.argmax(z)\n",
    "    t_shift = np.random.randint(negative_time_shifts[ind_label], positive_time_shifts[ind_label])\n",
    "  \n",
    "    data = np.roll(data, t_shift, axis=0)\n",
    "\n",
    "    if t_shift < 0:\n",
    "        data[t_shift:, :] = 0\n",
    "    elif t_shift > 0:\n",
    "        data[:t_shift, :] = 0\n",
    "\n",
    "    return data\n",
    "\n",
    "def tf_time_shift(data, z):\n",
    "    data_shape = data.shape\n",
    "    [data,] = tf.py_function(time_shift, [data, z], [tf.float32])\n",
    "    data.set_shape(data_shape)\n",
    "    return data\n",
    "\n",
    "def gaussian_noise(data):\n",
    "    noise_stations = np.random.choice(cfg.GAUSS_NOISE.shape[1], 96, replace=False)\n",
    "    random_noise = cfg.GAUSS_NOISE[:,noise_stations]\n",
    "    t_shift = np.random.randint(0,96)\n",
    "    random_noise = np.roll(random_noise, t_shift, axis=1)\n",
    "    t_shift = np.random.randint(0,1024)\n",
    "    random_noise = np.roll(random_noise, t_shift, axis=0)\n",
    "    random_noise = np.random.uniform(0.01, 0.2, 96)*random_noise\n",
    "    data = data/np.max(np.abs(data))\n",
    "    data = data+ random_noise\n",
    "    data = data/np.max(np.abs(data))\n",
    "    return data\n",
    "\n",
    "def tf_gaussian_noise(data):\n",
    "    data_shape = data.shape\n",
    "    [data,] = tf.py_function(gaussian_noise, [data], [tf.float32])\n",
    "    data.set_shape(data_shape)\n",
    "    return data\n",
    "\n",
    "def normalize(data):\n",
    "    data = data/np.max(np.abs(data))\n",
    "    return data\n",
    "\n",
    "def tf_normalize(data):\n",
    "    return tf.math.divide(data, tf.math.reduce_max(tf.math.abs(data)))\n",
    "\n",
    "def FCN_output(sx):\n",
    "    x = np.reshape(np.linspace(5500,8500,128), (128,1,1)) #np.reshape(np.linspace(5300,8700,88), (88,1,1))\n",
    "    y = np.reshape(np.linspace(3500,6100,96), (1,96,1)) #np.reshape(np.linspace(3300,6300,48), (1,48,1\n",
    "    z = np.reshape(np.linspace(1400,3600,64), (1,1,64)) # np.reshape(np.linspace(1200,3800,32), (1,1,32)\n",
    "    xc = np.round(sx[0])\n",
    "    yc = np.round(sx[1])\n",
    "    zc = np.round(sx[2])\n",
    "    fcn_out = np.exp(-((x-xc)**2/(2*200**2)+(y-yc)**2/(2*200**2)+(z-zc)**2/(2*200**2)))#np.exp(-((x-xc)**2+(y-yc)**2+(z-zc)**2)/(2*300**2))#np.exp(-((x-xc)**2/(2*200**2)+(y-yc)**2/(2*200**2)+(z-zc)**2/(2*200**2)))\n",
    "    fcn_out = fcn_out/np.max(fcn_out)\n",
    "    return fcn_out\n",
    "\n",
    "def tf_FCN_output(xc,yc,zc):\n",
    "    [fcn,] = tf.py_function(FCN_output, [[xc,yc,zc]], [tf.float32])\n",
    "    fcn.set_shape((128,96,64))\n",
    "    return fcn\n",
    "\n",
    "def FCN_output_perturbed(sx):\n",
    "    x = np.reshape(np.linspace(5500,8500,128), (128,1,1)) #np.reshape(np.linspace(5300,8700,88), (88,1,1))\n",
    "    y = np.reshape(np.linspace(3500,6100,96), (1,96,1)) #np.reshape(np.linspace(3300,6300,48), (1,48,1\n",
    "    z = np.reshape(np.linspace(1400,3600,64), (1,1,64)) # np.reshape(np.linspace(1200,3800,32), (1,1,32)\n",
    "\n",
    "    loc_errors = [sx[3], sx[4], sx[5]]\n",
    "\n",
    "    rx = np.random.normal(loc=sx[0], scale=np.std(np.arange(-int(loc_errors[0]),int(loc_errors[0])+1)))\n",
    "    ry = np.random.normal(loc=sx[1], scale=np.std(np.arange(-int(loc_errors[1]),int(loc_errors[1])+1)))\n",
    "    rz = np.random.normal(loc=sx[2], scale=np.std(np.arange(-int(loc_errors[2]),int(loc_errors[2])+1)))\n",
    "\n",
    "    while abs(rx-sx[0]) > loc_errors[0]:\n",
    "      rx = np.random.normal(loc=sx[0], scale=np.std(np.arange(-int(loc_errors[0]),int(loc_errors[0])+1)))\n",
    "    while abs(ry-sx[1]) > loc_errors[1]:\n",
    "      ry = np.random.normal(loc=sx[1], scale=np.std(np.arange(-int(loc_errors[1]),int(loc_errors[1])+1)))\n",
    "    while abs(rz-sx[2]) > loc_errors[2]:\n",
    "      rz = np.random.normal(loc=sx[2], scale=np.std(np.arange(-int(loc_errors[2]),int(loc_errors[2])+1)))\n",
    "\n",
    "    xc = rx\n",
    "    yc = ry\n",
    "    zc = rz\n",
    "\n",
    "    fcn_out = np.exp(-((x-xc)**2/(2*200**2)+(y-yc)**2/(2*200**2)+(z-zc)**2/(2*200**2)))#np.exp(-((x-xc)**2+(y-yc)**2+(z-zc)**2)/(2*300**2))#np.exp(-((x-xc)**2/(2*200**2)+(y-yc)**2/(2*200**2)+(z-zc)**2/(2*200**2)))\n",
    "    fcn_out = fcn_out/np.max(fcn_out)\n",
    "    return fcn_out\n",
    "\n",
    "def tf_FCN_output_perturbed(xc,yc,zc, rx, ry, rz):\n",
    "    [fcn,] = tf.py_function(FCN_output, [[xc,yc,zc, rx, ry, rz]], [tf.float32])\n",
    "    fcn.set_shape((128,96,64))\n",
    "    return fcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E56PTcihYrh5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Create an input function reading a file using the Dataset API\n",
    "def read_dataset(prefix, batch_size, cfg):\n",
    "  def _input_fn(example_serialized):\n",
    "    feature_map = {\n",
    "        \"x_true\": tf.io.FixedLenFeature(shape=[], dtype=tf.float32, default_value=-1),\n",
    "        \"y_true\": tf.io.FixedLenFeature(shape=[], dtype=tf.float32, default_value=-1),\n",
    "        \"z_true\": tf.io.FixedLenFeature(shape=[], dtype=tf.float32, default_value=-1),\n",
    "        \"sx_error\": tf.io.FixedLenFeature(shape=[], dtype=tf.float32, default_value=-1),\n",
    "        \"sy_error\": tf.io.FixedLenFeature(shape=[], dtype=tf.float32, default_value=-1),\n",
    "        \"sz_error\": tf.io.FixedLenFeature(shape=[], dtype=tf.float32, default_value=-1),\n",
    "        \"z_id\": tf.io.FixedLenFeature(shape=[6], dtype=tf.float32),\n",
    "        \"data\": tf.io.FixedLenFeature(shape=[cfg.N_TIMESAMPLES, cfg.N_TRACES], dtype=tf.float32),\n",
    "        }\n",
    "\n",
    "    parsed = tf.io.parse_single_example(example_serialized, feature_map)\n",
    "    data = parsed[\"data\"]\n",
    "\n",
    "    if tf.math.reduce_max(parsed[\"z_id\"])>=0:\n",
    "      data = tf_time_shift(data, parsed[\"z_id\"])\n",
    "      #data = tf_gaussian_noise(data)\n",
    "      data = tf_full_field_noise(data)\n",
    "      data = tf_station_dropout(data)\n",
    "      label = tf_FCN_output(parsed[\"x_true\"], parsed[\"y_true\"], parsed[\"z_true\"])\n",
    "    else:\n",
    "      label = tf_FCN_output(parsed[\"x_true\"], parsed[\"y_true\"], parsed[\"z_true\"])\n",
    "      #label = tf_FCN_output_perturbed(parsed[\"x_true\"], parsed[\"y_true\"], parsed[\"z_true\"],\n",
    "      #                                parsed[\"sx_error\"], parsed[\"sy_error\"], parsed[\"sz_error\"])\n",
    "\n",
    "    data = tf_normalize(data)\n",
    "    data = tf.reshape(data, (cfg.N_TIMESAMPLES, cfg.N_TRACES, 1))\n",
    "    \n",
    "    return (data, label)\n",
    "\n",
    "  def _input_fn2(example_serialized):\n",
    "    feature_map = {\n",
    "        \"x_true\": tf.io.FixedLenFeature(shape=[], dtype=tf.float32, default_value=-1),\n",
    "        \"y_true\": tf.io.FixedLenFeature(shape=[], dtype=tf.float32, default_value=-1),\n",
    "        \"z_true\": tf.io.FixedLenFeature(shape=[], dtype=tf.float32, default_value=-1),\n",
    "        \"z_id\": tf.io.FixedLenFeature(shape=[6], dtype=tf.float32),\n",
    "        \"data\": tf.io.FixedLenFeature(shape=[cfg.N_TIMESAMPLES, cfg.N_TRACES], dtype=tf.float32),\n",
    "        }\n",
    "\n",
    "    parsed = tf.io.parse_single_example(example_serialized, feature_map)\n",
    "    data = parsed[\"data\"]\n",
    "\n",
    "    label = tf_FCN_output(parsed[\"x_true\"], parsed[\"y_true\"], parsed[\"z_true\"])\n",
    "\n",
    "    data = tf_normalize(data)\n",
    "    data = tf.reshape(data, (cfg.N_TIMESAMPLES, cfg.N_TRACES, 1))\n",
    "    \n",
    "    return (data, label)\n",
    "\n",
    "  if prefix==\"train\":\n",
    "\n",
    "    # Get synthetic and field data\n",
    "    file_path_synth = os.path.join(cfg.PATH_TO_SYNTH_DATA,'%s*' % prefix)\n",
    "    file_path_field = os.path.join(cfg.PATH_TO_FIELD_DATA, '%s*' % prefix)\n",
    "\n",
    "    # Create list of files that match pattern and process synthetic dataset\n",
    "    file_list_synth = tf.io.matching_files(file_path_synth)\n",
    "    file_list_synth = tf.random.shuffle(file_list_synth)\n",
    "    file_list_field = tf.io.matching_files(file_path_field)\n",
    "    if cfg.TRANSFER_LEARNING == True:\n",
    "      #file_list_synth = tf.random.shuffle(file_list_synth)\n",
    "      file_list_synth = tf.slice(file_list_synth, [0], [cfg.N_SYNTH_FILES])\n",
    "      file_list = tf.concat([file_list_synth, file_list_field], axis=-1) # combine field and synthetic data list\n",
    "      shards = tf.data.Dataset.from_tensor_slices(file_list)\n",
    "      shards = shards.shuffle(tf.cast(tf.shape(file_list)[0], tf.int64))\n",
    "      shards = shards.repeat()\n",
    "      #dataset = dataset.map(_input_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "      #ataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "    else:\n",
    "      file_list = tf.concat([file_list_synth, file_list_field], axis=-1)\n",
    "      shards = tf.data.Dataset.from_tensor_slices(file_list)\n",
    "      shards = shards.shuffle(tf.cast(tf.shape(file_list)[0], tf.int64))\n",
    "      shards = shards.repeat()\n",
    "    # Feed the shards into TFRecordDataset and randomize again with interleave.\n",
    "    dataset = shards.interleave(tf.data.TFRecordDataset, cycle_length=4, block_length=16, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(buffer_size=10*batch_size)\n",
    "    num_epochs=None\n",
    "    dataset = dataset.map(_input_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "\n",
    "\n",
    "  if prefix == \"val\":\n",
    "    file_path_field = os.path.join(cfg.PATH_TO_FIELD_DATA, '%s*' % prefix)\n",
    "    file_list_field = tf.io.matching_files(file_path_field)\n",
    "    shards = tf.data.Dataset.from_tensor_slices(file_list_field)\n",
    "    dataset = tf.data.TFRecordDataset(shards)\n",
    "    num_epochs = 1\n",
    "    dataset = dataset.map(_input_fn2, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "\n",
    "  return dataset.prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HA2kXmnPBEq9"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Create an input function reading a file using the Dataset API\n",
    "def read_dataset_field(prefix, batch_size, cfg):\n",
    "  def _input_fn(example_serialized):\n",
    "    feature_map = {\n",
    "        \"x_true\": tf.io.FixedLenFeature(shape=[], dtype=tf.float32, default_value=-1),\n",
    "        \"y_true\": tf.io.FixedLenFeature(shape=[], dtype=tf.float32, default_value=-1),\n",
    "        \"z_true\": tf.io.FixedLenFeature(shape=[], dtype=tf.float32, default_value=-1),\n",
    "        \"sx_error\": tf.io.FixedLenFeature(shape=[], dtype=tf.float32, default_value=-1),\n",
    "        \"sy_error\": tf.io.FixedLenFeature(shape=[], dtype=tf.float32, default_value=-1),\n",
    "        \"sz_error\": tf.io.FixedLenFeature(shape=[], dtype=tf.float32, default_value=-1),\n",
    "        \"z_id\": tf.io.FixedLenFeature(shape=[6], dtype=tf.float32),\n",
    "        \"data\": tf.io.FixedLenFeature(shape=[cfg.N_TIMESAMPLES, cfg.N_TRACES], dtype=tf.float32),\n",
    "        }\n",
    "\n",
    "    parsed = tf.io.parse_single_example(example_serialized, feature_map)\n",
    "    data = parsed[\"data\"]\n",
    "\n",
    "    label = tf_FCN_output(parsed[\"x_true\"], parsed[\"y_true\"], parsed[\"z_true\"])\n",
    "    data = tf_normalize(data)\n",
    "    data = tf.reshape(data, (cfg.N_TIMESAMPLES, cfg.N_TRACES, 1))\n",
    "    \n",
    "    return (data, label)\n",
    "\n",
    "  if prefix==\"train\":\n",
    "\n",
    "    # Get synthetic and field data\n",
    "    file_path_field = os.path.join(cfg.PATH_TO_FIELD_DATA, '%s*' % prefix)\n",
    "\n",
    "    # Create list of files that match pattern and process synthetic dataset\n",
    "    file_list_field = tf.io.matching_files(file_path_field)\n",
    "    if cfg.TRANSFER_LEARNING == True:\n",
    "      shards = tf.data.Dataset.from_tensor_slices(file_list_field)\n",
    "      shards = shards.shuffle(tf.cast(tf.shape(file_list_field)[0], tf.int64))\n",
    "      shards = shards.repeat()\n",
    "    else:\n",
    "      shards = tf.data.Dataset.from_tensor_slices(file_list_field)\n",
    "      shards = shards.shuffle(tf.cast(tf.shape(file_list)[0], tf.int64))\n",
    "      shards = shards.repeat()\n",
    "    # Feed the shards into TFRecordDataset and randomize again with interleave.\n",
    "    dataset = shards.interleave(tf.data.TFRecordDataset, cycle_length=4, block_length=16, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(buffer_size=10*batch_size)\n",
    "    num_epochs=None\n",
    "    dataset = dataset.map(_input_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "\n",
    "\n",
    "  if prefix == \"val\":\n",
    "    file_path_field = os.path.join(cfg.PATH_TO_FIELD_DATA, '%s*' % prefix)\n",
    "    file_list_field = tf.io.matching_files(file_path_field)\n",
    "    shards = tf.data.Dataset.from_tensor_slices(file_list_field)\n",
    "    dataset = tf.data.TFRecordDataset(shards)\n",
    "    num_epochs = 1\n",
    "    dataset = dataset.map(_input_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "\n",
    "  return dataset.prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5nD66oxFBE2s"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Create an input function reading a file using the Dataset API\n",
    "def read_dataset_synth(prefix, batch_size, cfg):\n",
    "  def _input_fn(example_serialized):\n",
    "    feature_map = {\n",
    "        \"x_true\": tf.io.FixedLenFeature(shape=[], dtype=tf.float32, default_value=-1),\n",
    "        \"y_true\": tf.io.FixedLenFeature(shape=[], dtype=tf.float32, default_value=-1),\n",
    "        \"z_true\": tf.io.FixedLenFeature(shape=[], dtype=tf.float32, default_value=-1),\n",
    "        \"sx_error\": tf.io.FixedLenFeature(shape=[], dtype=tf.float32, default_value=-1),\n",
    "        \"sy_error\": tf.io.FixedLenFeature(shape=[], dtype=tf.float32, default_value=-1),\n",
    "        \"sz_error\": tf.io.FixedLenFeature(shape=[], dtype=tf.float32, default_value=-1),\n",
    "        \"z_id\": tf.io.FixedLenFeature(shape=[6], dtype=tf.float32),\n",
    "        \"data\": tf.io.FixedLenFeature(shape=[cfg.N_TIMESAMPLES, cfg.N_TRACES], dtype=tf.float32),\n",
    "        }\n",
    "\n",
    "    parsed = tf.io.parse_single_example(example_serialized, feature_map)\n",
    "    data = parsed[\"data\"]\n",
    "\n",
    "    data = tf_time_shift(data, parsed[\"z_id\"])\n",
    "    data = tf_full_field_noise(data)\n",
    "    data = tf_station_dropout(data)\n",
    "    label = tf_FCN_output(parsed[\"x_true\"], parsed[\"y_true\"], parsed[\"z_true\"])\n",
    "    data = tf_normalize(data)\n",
    "    data = tf.reshape(data, (cfg.N_TIMESAMPLES, cfg.N_TRACES, 1))\n",
    "    \n",
    "    return (data, label)\n",
    "\n",
    "  if prefix==\"train\":\n",
    "\n",
    "    # Get synthetic and field data\n",
    "    file_path_synth = os.path.join(cfg.PATH_TO_SYNTH_DATA,'%s*' % prefix)\n",
    "\n",
    "    # Create list of files that match pattern and process synthetic dataset\n",
    "    file_list_synth = tf.io.matching_files(file_path_synth)\n",
    "    file_list_synth = tf.random.shuffle(file_list_synth)\n",
    "    if cfg.TRANSFER_LEARNING == True:\n",
    "      file_list_synth = tf.slice(file_list_synth, [0], [cfg.N_SYNTH_FILES])\n",
    "      shards = tf.data.Dataset.from_tensor_slices(file_list_synth)\n",
    "      shards = shards.shuffle(tf.cast(tf.shape(file_list_synth)[0], tf.int64))\n",
    "      shards = shards.repeat()\n",
    "    else:\n",
    "      shards = tf.data.Dataset.from_tensor_slices(file_list_synth)\n",
    "      shards = shards.shuffle(tf.cast(tf.shape(file_list)[0], tf.int64))\n",
    "      shards = shards.repeat()\n",
    "    # Feed the shards into TFRecordDataset and randomize again with interleave.\n",
    "    dataset = shards.interleave(tf.data.TFRecordDataset, cycle_length=4, block_length=16, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(buffer_size=10*batch_size)\n",
    "    num_epochs=None\n",
    "    dataset = dataset.map(_input_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "\n",
    "\n",
    "  if prefix == \"val\":\n",
    "    file_path_field = os.path.join(cfg.PATH_TO_FIELD_DATA, '%s*' % prefix)\n",
    "    file_list_field = tf.io.matching_files(file_path_field)\n",
    "    shards = tf.data.Dataset.from_tensor_slices(file_list_field)\n",
    "    dataset = tf.data.TFRecordDataset(shards)\n",
    "    num_epochs = 1\n",
    "    dataset = dataset.map(_input_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "\n",
    "  return dataset.prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4gqBBRXhZDyZ"
   },
   "outputs": [],
   "source": [
    "def dice_coef_gaussian(y_true, y_pred, smooth=1):\n",
    "  y_true = tf.dtypes.cast(y_true>0.1, tf.int32)\n",
    "  y_true = tf.dtypes.cast(y_true, tf.float32)\n",
    "  y_pred = tf.dtypes.cast(y_pred>0.1, tf.int32)\n",
    "  y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "  intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
    "  union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n",
    "  dice = K.mean((2. * intersection + smooth)/(union + smooth), axis=0)\n",
    "  return dice\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "  y_true = tf.dtypes.cast(y_true, tf.float32)\n",
    "  y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "  intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
    "  union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n",
    "  dice = K.mean((2. * intersection + smooth)/(union + smooth), axis=0)\n",
    "  return dice\n",
    "\n",
    "# Clip values in Gaussian above 0.1 to 1 and below 0.1 to 0\n",
    "def iou_coef_gaussian(y_true, y_pred, smooth=1):\n",
    "    y_true = tf.dtypes.cast(y_true>0.1, tf.int32)\n",
    "    y_true = tf.dtypes.cast(y_true, tf.float32)\n",
    "    y_pred = tf.dtypes.cast(y_pred>0.1, tf.int32)\n",
    "    y_pred = tf.dtypes.cast(y_pred, tf.float32)\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=[1,2,3])\n",
    "    union = K.sum(y_true,[1,2,3])+K.sum(y_pred,[1,2,3])-intersection\n",
    "    iou = K.mean((intersection + smooth) / (union + smooth), axis=0)\n",
    "    return iou\n",
    "\n",
    "def train_and_evaluate(cfg):\n",
    "\n",
    "\n",
    "  checkpoint_filepath = 'drive/My Drive/Texas_TL_sourceLocalization/checkpoint'\n",
    "  model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "      filepath=checkpoint_filepath,\n",
    "      save_weights_only=True,\n",
    "      monitor='val_dice_coef',\n",
    "      mode='max',\n",
    "      save_best_only=True\n",
    "  )\n",
    "  print(\"loading model...\")\n",
    "  generator = tf.keras.models.load_model(\n",
    "      os.path.join(cfg.PATH_TO_MODELS, cfg.LOAD_MODEL),\n",
    "      custom_objects={\n",
    "          'ReLU':tf.keras.layers.ReLU,\n",
    "          'dice_coef_gaussian': dice_coef_gaussian, 'iou_coef_gaussian': iou_coef_gaussian,\n",
    "          })\n",
    "  \n",
    "  generator.compile(\n",
    "      optimizer=tf.keras.optimizers.Adam(learning_rate=cfg.LEARNING_RATE),\n",
    "      loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "      metrics=[dice_coef_gaussian, dice_coef])#metrics=['accuracy', iou_coef_gaussian, dice_coef_gaussian]\n",
    "\n",
    "  # Compute train and validation steps\n",
    "  file_path = os.path.join(cfg.PATH_TO_SYNTH_DATA,'%s*' % cfg.TRAIN)\n",
    "  file_list = tf.io.matching_files(file_path)\n",
    "\n",
    "  file_list = file_list[:cfg.N_SYNTH_FILES]\n",
    "  n_train_examples_synth = len(file_list)*cfg.N_FILES_SYNTH_TFRECORD \n",
    "  n_train_files_field = cfg.N_FILES_FIELD_TFRECORD_TRAIN\n",
    "  n_train_examples = n_train_files_field + n_train_examples_synth\n",
    "\n",
    "  steps_per_epoch = n_train_examples // cfg.BATCH_SIZE\n",
    "  val_steps = cfg.N_FILES_FIELD_TFRECORD_VAL // cfg.BATCH_SIZE\n",
    "  \n",
    "  for i in range(100):\n",
    "    # Load the training data\n",
    "    print(i)\n",
    "    trainds = read_dataset(prefix=cfg.TRAIN, batch_size=cfg.BATCH_SIZE, cfg=cfg)\n",
    "    valds = read_dataset(prefix=cfg.VAL, batch_size=cfg.BATCH_SIZE, cfg=cfg)\n",
    "    generator.fit(\n",
    "        trainds,\n",
    "        validation_data=valds,\n",
    "        epochs=cfg.N_EPOCHS,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_steps=val_steps,\n",
    "        verbose=1,\n",
    "        callbacks=[model_checkpoint_callback])\n",
    "    \n",
    "    save_as = os.path.join(cfg.PATH_TO_TL_MODELS, \"{}_{}.h5\".format(cfg.MODEL_NAME, i))\n",
    "    generator.save(save_as, overwrite=True, include_optimizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pvItUmc0Qz1E"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "files = glob.glob(\"drive/My Drive/Texas_TL/Data/contNoise_newShape/*\")\n",
    "\n",
    "#field_noise = np.load(files[0])\n",
    "#n_size = field_noise.shape\n",
    "#field_noise = np.reshape(field_noise, (1, n_size[0], n_size[1]))\n",
    "#for i, fn in enumerate(files[1:]):\n",
    "#  field_noise = np.append(field_noise, np.reshape(np.load(fn), (1,n_size[0], n_size[1])), axis=0)\n",
    "\n",
    "class Config(object):\n",
    "  def __init__(self):\n",
    "    # PATHS TO MODELS AND OUTPUT\n",
    "    self.PATH_TO_SYNTH_DATA = \"drive/My Drive/Texas_TL/Data/TFR_new\"\n",
    "    self.PATH_TO_FIELD_DATA = \"drive/My Drive/Texas_TL/Data/TFR_field\"#_days4\"\n",
    "    self.PATH_TO_MODELS = \"drive/My Drive/Texas_TL/Models/original_QNet\"\n",
    "    self.PATH_FIELD_NOISE = \"drive/My Drive/Texas_TL/Data/contNoise_newShape\"\n",
    "    self.PATH_TO_TL_MODELS = \"drive/My Drive/Texas_TL/TL_models/TL_TFR_regular_updating\"\n",
    "    # DEFINE SIZE OF INPUT, BATCH SIZE, NUMBER OF EPOCHS (EVALS), LEARNING RATE, OTHER FIXED NUMBERS\n",
    "    self.BATCH_SIZE = 10\n",
    "    self.N_TRACES = 96\n",
    "    self.N_TIMESAMPLES = 1024\n",
    "    self.N_EPOCHS = 4\n",
    "    self.LEARNING_RATE =  0.001\n",
    "    self.N_FILES_SYNTH_TFRECORD = 400\n",
    "    self.N_FILES_FIELD_TFRECORD_TRAIN = 747 #747 for all train set # 357  #477 for combo_train_val\n",
    "    self.N_FILES_FIELD_TFRECORD_VAL = 249 # 249 #  120\n",
    "    self.MIN_DROP = 5\n",
    "    self.MAX_DROP = 25\n",
    "    # NAME FOR SAVING THE MODEL\n",
    "    self.MODEL_NAME = \"QNet_post_test_long\" #\"QNet-Synth2000-noPrtbn-\"\n",
    "    # PREFIX FOR TRAINING AND VALIDATION SETS\n",
    "    self.TRAIN = \"train\"\n",
    "    self.VAL = \"val\"\n",
    "    # LOAD FIELD NOISE, SINGLE FIELD NOISE AND GAUSSIAN NOISE\n",
    "    self.field_noise = field_noise\n",
    "    # NAME FOR CHECKPOINTS\n",
    "    self.LOAD_MODEL = \"QNet_FieldGaus8t_20ep_20210324182729.h5\" # name of model to load before starting training\n",
    "    self.TRANSFER_LEARNING = True\n",
    "    self.N_SYNTH_FILES = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LiHWAo5UQuuz"
   },
   "outputs": [],
   "source": [
    "cfg = Config()\n",
    "train_and_evaluate(cfg)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "practical_TL_TFrecords.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
